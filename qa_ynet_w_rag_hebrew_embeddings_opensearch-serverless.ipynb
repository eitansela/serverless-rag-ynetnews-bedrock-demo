{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question & Answering on Ynet data in Hebrew with Amazon Bedrock using LangChain & Amazon OpenSearch\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "Previously we saw that the model told us how to to change the tire, however we had to manually provide it with the relevant data and provide the contex ourselves. We explored the approach to leverage the model availabe under Bedrock and ask questions based on it's knowledge learned during training as well as providing manual context. While that approach works with short documents or single-ton applications, it fails to scale to enterprise level question answering where there could be large enterprise documents which cannot all be fit into the prompt sent to the model. \n",
    "\n",
    "### Pattern\n",
    "We can improve upon this process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "In this notebook we explain how to approach the pattern of Question Answering to find and leverage the documents to provide answers to the user questions.\n",
    "\n",
    "### Challenges\n",
    "- How to manage large document(s) that exceed the token limit\n",
    "- How to find the document(s) relevant to the question being asked\n",
    "\n",
    "### Proposal\n",
    "To the above challenges, this notebook proposes the following strategy\n",
    "#### Prepare documents\n",
    "![Embeddings](./images/Embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Scrape [Ynet](https://www.ynet.co.il/home/0,7340,L-8,00.html) news site.\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "#### Ask question\n",
    "![Question](./images/Chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In order to follow the RAG approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Anthropic Claude V2 available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "- **Ynet scrapping function**: a function to scrapes articles from Ynet news site using beautifulsoup4 Python package \n",
    "\n",
    "- **Vector Store**: OpenSearch available through LangChain\n",
    "\n",
    "  In this notebook we are using Amazon OpenSearch as a vector-store to store both the embeddings and the documents. \n",
    "- **Index**: VectorIndex\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock.\n",
    "\n",
    "In this notebook, we'll also need some extra dependencies:\n",
    "\n",
    "- [OpenSearch Python Client](https://pypi.org/project/opensearch-py/), to store vector embeddings\n",
    "- [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to scrape the Ynet news site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting boto3>=1.28.57\n",
      "  Obtaining dependency information for boto3>=1.28.57 from https://files.pythonhosted.org/packages/c7/dd/4fe47b2cec8731ec26d7410e659c4f0c4cd36baa835e2312cb0ec5383b07/boto3-1.28.65-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.28.65-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting awscli>=1.29.57\n",
      "  Obtaining dependency information for awscli>=1.29.57 from https://files.pythonhosted.org/packages/0e/d2/d47172d6159f07255cf07f28772d9d6536ba749432a2f566aa515c59094a/awscli-1.29.65-py3-none-any.whl.metadata\n",
      "  Downloading awscli-1.29.65-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting botocore>=1.31.57\n",
      "  Obtaining dependency information for botocore>=1.31.57 from https://files.pythonhosted.org/packages/63/c6/8e29a2b9dffa188d07c26d19ae578a26d8063834e4d844bf22c2a0028229/botocore-1.31.65-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.31.65-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.57)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3>=1.28.57)\n",
      "  Obtaining dependency information for s3transfer<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/5a/4b/fec9ce18f8874a96c5061422625ba86c3ee1e6587ccd92ff9f5bf7bd91b2/s3transfer-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting docutils<0.17,>=0.10 (from awscli>=1.29.57)\n",
      "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML<6.1,>=3.10 (from awscli>=1.29.57)\n",
      "  Obtaining dependency information for PyYAML<6.1,>=3.10 from https://files.pythonhosted.org/packages/29/61/bf33c6c85c55bc45a29eee3195848ff2d518d84735eb0e2d8cb42e0d285e/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting colorama<0.4.5,>=0.2.5 (from awscli>=1.29.57)\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.29.57)\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore>=1.31.57)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m309.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<2.1,>=1.25.4 (from botocore>=1.31.57)\n",
      "  Obtaining dependency information for urllib3<2.1,>=1.25.4 from https://files.pythonhosted.org/packages/d2/b2/b157855192a68541a91ba7b2bbcb91f1b4faa51f8bae38d8005c034be524/urllib3-2.0.7-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore>=1.31.57)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli>=1.29.57)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m184.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.28.65-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m276.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading awscli-1.29.65-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.65-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m251.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m332.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m269.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m267.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.0\n",
      "    Uninstalling pyasn1-0.5.0:\n",
      "      Successfully uninstalled pyasn1-0.5.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.7.2\n",
      "    Uninstalling rsa-4.7.2:\n",
      "      Successfully uninstalled rsa-4.7.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.65\n",
      "    Uninstalling botocore-1.31.65:\n",
      "      Successfully uninstalled botocore-1.31.65\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.7.0\n",
      "    Uninstalling s3transfer-0.7.0:\n",
      "      Successfully uninstalled s3transfer-0.7.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.28.65\n",
      "    Uninstalling boto3-1.28.65:\n",
      "      Successfully uninstalled boto3-1.28.65\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.29.65\n",
      "    Uninstalling awscli-1.29.65:\n",
      "      Successfully uninstalled awscli-1.29.65\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "nemoguardrails 0.5.0 requires langchain==0.0.251, but you have langchain 0.0.309 which is incompatible.\n",
      "opensearch-py 2.3.1 requires urllib3<2,>=1.21.1, but you have urllib3 2.0.7 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.2.2 which is incompatible.\n",
      "pyasn1-modules 0.2.8 requires pyasn1<0.5.0,>=0.4.6, but you have pyasn1 0.5.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.15.0 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.0a7 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 awscli-1.29.65 boto3-1.28.65 botocore-1.31.65 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 pyasn1-0.5.0 python-dateutil-2.8.2 rsa-4.7.2 s3transfer-0.7.0 six-1.16.0 urllib3-2.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opensearch-py==2.3.1 in /opt/conda/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: langchain==0.0.309 in /opt/conda/lib/python3.10/site-packages (0.0.309)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.2 in /opt/conda/lib/python3.10/site-packages (4.12.2)\n",
      "Collecting urllib3<2,>=1.21.1 (from opensearch-py==2.3.1)\n",
      "  Obtaining dependency information for urllib3<2,>=1.21.1 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /opt/conda/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2023.7.22)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (3.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.40 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (0.0.43)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (1.10.13)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.309) (8.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4==4.12.2) (2.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.309) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.309) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.309) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.309) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.309) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.309) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.309) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.309) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.309) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.309) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.309) (2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.309) (4.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.309) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.309) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.309) (0.4.3)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m164.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-1.26.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U opensearch-py==2.3.1 langchain==0.0.309 beautifulsoup4==4.12.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "boto3_bedrock = boto3.client(service_name='bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure langchain\n",
    "\n",
    "We begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude for text generation and Amazon Titan for text embedding.\n",
    "\n",
    "Note: It is possible to choose other models available with Bedrock. You can replace the `model_id` as follows to change the model.\n",
    "\n",
    "`llm = Bedrock(model_id=\"amazon.titan-tg1-large\")`\n",
    "\n",
    "Available model IDs include:\n",
    "\n",
    "- `ai21.j2-ultra-v1`\n",
    "- `ai21.j2-mid-v1`\n",
    "- `amazon.titan-embed-text-v1`\n",
    "- `amazon.titan-text-express-v1`\n",
    "- `anthropic.claude-v1`\n",
    "- `anthropic.claude-v2`\n",
    "- `anthropic.claude-instant-v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.load.dump import dumps\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 4096}\n",
    ")\n",
    "\n",
    "# - create the Titan embeddings Model\n",
    "embed_model_id = \"amazon.titan-embed-text-v1\"\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=embed_model_id, client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's first define a function to scrape Ynet news site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_articles_from_ynet():\n",
    "    url = \"https://www.ynet.co.il\"\n",
    "    articles = []\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    sub_links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if \"article\" in str(a['href']):\n",
    "            sub_links.append(a['href'])\n",
    "\n",
    "    sub_links = set(sub_links)\n",
    "    # print(sub_links)\n",
    "\n",
    "    print(f\"Importing articles from: {url}. Found {len(sub_links)} sub links.\")\n",
    "    for link in tqdm(sub_links):\n",
    "        page = requests.get(link)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        data = soup.find('script', attrs={'type': 'application/ld+json'})\n",
    "        data = str(data)\n",
    "        data = data.split(\"{\", 1)[1]\n",
    "        d = data.strip(\"</script>\")\n",
    "        d = \"{\" + d\n",
    "        try:\n",
    "            metaData = json.loads(d)\n",
    "        except Exception as e:\n",
    "            # print(\"Error loading article meta-data to json \", e)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            authors = metaData['author']['name'].split(',')  # to list\n",
    "            keywords = metaData['keywords'].split(',')  # to list\n",
    "            date = metaData['datePublished']\n",
    "\n",
    "            article = {\n",
    "                'title': metaData['headline'],\n",
    "                'date_published': date,\n",
    "                'authors': authors,\n",
    "                'link': link,\n",
    "                'keywords': keywords,\n",
    "                'summary': metaData['description'],\n",
    "                'text': metaData['articleBody'],\n",
    "                'link': link\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # print(\"Error loading article data \", e)\n",
    "            continue\n",
    "\n",
    "        if article['text']:\n",
    "            articles.append(article)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets run the scraping function. It should take around 2 minutes to complete.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing articles from: https://www.ynet.co.il. Found 135 sub links.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [01:39<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.56 s, sys: 164 ms, total: 7.73 s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles = get_articles_from_ynet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the fetched Ynet articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ממחדשים: כדורי סושי מהאורז של אתמול וביסלי ביתי משאריות הפסטה \n",
      "text: כדורי סושי    זהו מתכון ניצול שאריות מושלם מכיוון שהוא מאפשר גיוון רב. אם נשארו לכם דגים משבת, אתם יכולים לפרק אותם ולהשתמש במקום הטונה ולנצל את הירקות שיש בבית למילוי. ערכים תזונתיים ליחידה: 72 קלוריות | 9.5 גרם פחמימות | 3 גרם חלבון | 2 גרם שומן |  12 יחידות החומרים: לאורז: 2 כוסות אורז מבושל מכל סוג  2 כפות מירין (או 1 כף חומץ + 1 כף מייפל) 1/2 כפית מלח למילוי: 1 קופסת טונה בשמן, מסוננת  1 מלפפון קצוץ דק 1 גזר קצוץ דק 1 אבוקדו חתוך לקוביות קטנות  1 גבעול בצל ירוק, קצוץ  לקישוט: שומשום שחור ולבן  אופן ההכנה: מניחים את האורז בקערה המתאימה למיקרו ומוסיפים 1/2 כוס מים רותחים. סוגרים ומחממים 4 דקות במיקרו. משאירים סגור 5 דקות נוספות ומוציאים.  מערבבים את האורז עם כל שאר המרכיבים.  מניחים שכבת אורז על ניילון נצמד או על דף שעווה. מסדרים מעל מעט מילוי בהרכב שאתם אוהבים במרכז שכבת האורז. סוגרים מכל הצדדים לכדור בעזרת הניילון הנצמד ומהדקים היטב.  משחררים את כדור האורז בעדינות, מפזרים מעט שומשום ומניחים במקרר. מכינים כך את שאר הכדורים.  עוד מתכונים מהירים ובריאים: ארוחות קיץ לילדים: שניצל, פתיתים וגלידה - הגרסה הבריאה צ'יפס בריא ופיצה קלילה על בסיס ירקות  מתכון בריא בשבוע: שניצל טופו ברוקולי בתנור ספגטי משאריות אנטיפסטי   נשארו לכם ירקות נוספים מהאנטיפסטי שהכנתם? אל תחששו לערבב ולשלב. מכיוון שטוחנים אותם יחד לרוטב, הטעמים יתמזגו. ערכים תזונתיים למנה: 454 קלוריות | 79 גרם פחמימות | 15 גרם חלבון | 6.5 גרם שומן |  5 מנות החומרים: 1 חבילת ספגטי (500 גרם) לרוטב: 4־5 פלפלים אנטיפסטי  2 בצלים סגולים או בצלים מהאנטיפסטי  2 שיני שום  1 כפית מלח  1/4 כפית פלפל שחור  1/4 כפית צ׳ילי  1 חבילת שמנת לבישול 10% אופן ההכנה: מבשלים את הספגטי לפי הוראות היצרן ושומרים את מי הפסטה בצד. מעבירים את כל מרכיבי הרוטב לבלנדר חזק או למעבד מזון וטוחנים למרקם חלק.  מבשלים את הרוטב במחבת רחבה, טועמים ומתקנים תיבול. מוסיפים מעט ממי הפסטה במידת הצורך.  מוסיפים את הפסטה למחבת, מערבבים ומגישים.  לביבות קינואה שאריות   הכנתם יותר מדי קינואה כי תכננתם לאכול בריא כל השבוע אבל התחיל לשעמם לכם? הנה רעיון למנה שהיא גם אפויה וגם עשירה בירקות. ערכים תזונתיים ליחידה: 40 קלוריות | 3 גרם פחמימות | 2 גרם חלבון | 2 גרם שומן |  20 יחידות החומרים: 1 כוס קינואה מבושלת מכל סוג  2 ביצים  1 קישוא מגורר 1 גזר מגורר 1/2 כוס גבינת מוצרלה מגוררת (לא חובה) 1/3 כוס פטרוזיליה קצוצה 1/2 כפית מלח  1/4 כפית פלפל שחור  1 כף שמן לשימון אופן ההכנה: מערבבים את כל המרכיבים, פרט לשמן, לתערובת אחידה.  יוצרים מהתערובת לביבות ומניחים בתבנית מרופדת בנייר אפייה.  משמנים את הלביבות בעזרת תרסיס שמן או במברשת מטבח.  אופים בתנור שחומם מראש ל־180 מעלות בתוכנית טורבו כ־25־30 דקות, או עד שהלביבות מתייצבות ומזהיבות. טיפ: לא צריך לקלף את הירקות. אפשר לשטוף היטב וכך לשמור על הסיבים התזונתיים. ביסלי פלאפל ביתי ערכים תזונתיים למנה: 179 קלוריות | 23.5 גרם פחמימות | 4.5 גרם חלבון | 7 גרם שומן |  8 מנות החומרים:  1/2 חבילת פסטה פנה מבושלת (250 גרם לפני בישול) 1/4 כוס שמן זית  1 כף כמון 1 כפית פפריקה מתוקה 1/4 כפית כורכום 1/2 כפית מלח אופן ההכנה: מערבבים את הפסטה עם שמן הזית וכל התבלינים.  אופים בתנור שחומם מראש ל־220 מעלות כ־20 דקות. מערבבים מדי פעם תוך כדי.   ביסלי גריל ביתי  ערכים תזונתיים למנה: 190 קלוריות | 24.5 גרם פחמימות | 5 גרם חלבון | 7.5 גרם שומן |  8 מנות החומרים: 1/2 חבילת פסטה פוזילי מקמח מלא, מבושלת (250 גרם לפני בישול)  1/4 כוס שמן זית  2 כפות פפריקה מתוקה 2 כפות תבלין \"על האש\"  1 כפית גדושה פפריקה מעושנת  אופן ההכנה: מערבבים את הפסטה עם שמן הזית וכל התבלינים.  אופים בתנור שחומם מראש ל־220 מעלות כ־20 דקות. מערבבים מדי פעם תוך כדי. נשארו לכם שאריות של פסטה מבושלת? הכינו ביסלי ביתי בטעמים. העיקרון זהה: 1/4 כוס שמן זית ל־1/2 חבילת פסטה מבושלת (250 גרם לפני בישול). התבלינים עליכם. עוגיות בננה מארבעה מרכיבים   מתכון קל ומהיר עם בננות שהבשילו יתר על המידה, ואפילו לא צריך רכיבים כמו קמח וחמאה! ערכים תזונתיים ליחידה: 98 קלוריות | 15 גרם פחמימות | 3 גרם חלבון | 2.5 גרם שומן |  15 עוגיות החומרים: 2 בננות בינוניות, רכות 1 כוס שיבולת שועל עבה 1/2 כוס שיבולת שועל דקה אינסטנט 1/3 כוס שוקולד צ'יפס מריר אופן ההכנה: מועכים בקערה בינונית את הבננות לעיסה חלקה במרסק תפוחי אדמה או במזלג. מוסיפים לקערה את כל המרכיבים הנוספים ומערבבים לתערובת אחידה. יוצרים עוגיות בעזרת 2 כפות ומסדרים בתבנית תנור מרופדת בנייר אפייה (העוגיות לא משתנות מאוד באפייה. אפשר ליצור אותן מעט שטוחות). אופים את העוגיות בתנור שחומם מראש ל־180 מעלות בתוכניות טורבו במשך כ־12־17 דקות או עד שהן מזהיבות בצדדים.    7 טיפים מנצחים של אריאל לצמצום בזבוז המזון נסו לדחות את הקניות תמיד בעוד יום. לא תאמינו אילו ארוחות מגוונות ויצירתיות יֵצאו לכם כשתזוזו קצת מאזור הנוחות ותנצלו שאריות. ציידו את המקפיא. כשיוצאת כמות אוכל גדולה, כדאי להשאיר תמיד קופסה אחת במקפיא שתהיה מנה ליום שלא הספקתם לבשל בו.  שמרו את האוכל בקופסאות שקופות. כך תמיד תראו מה יש במקרר ולא תפספסו מצרכים. השתדלו לא להשאיר פירות וירקות בשקיות במקרר. במקום זאת כדאי לשים אותם בגובה העיניים כדי שזאת תמיד תהיה הבחירה הראשונה שלכם כשתפתחו את המקרר. גם בריא יותר וגם טרי יותר. נצלו את הפירות הבשלים. פירות רכים יותר ומושכים פחות כדאי להעביר ישר למקפיא וביום קיץ חם לשלוף ולהכין מהם גלידה שווה. חפשו השראה. כדי לקבל רעיונות אפשר לעקוב ברשתות החברתיות אחרי חשבונות ועמודים שמעודדים שימוש יצירתי ומגוון באוכל רגע לפני שהוא נזרק. קנו פחות. זה תמיד הטיפ מספר אחת... זכרו שעדיף להשלים קנייה מאשר לזרוק.  שפית אורחת: אריאל בן חמו, בריאלי ///  חישוב ערכים תזונתיים: איילת מלניק, תזונאית קלינית כללית, מחוז ירושלים\n",
      "link: https://www.ynet.co.il/menta/article/s111jpbt11a\n",
      "date_published: 2023-10-16T06:14:12.798464Z\n",
      "summary: מצאתם במקרר אורז משבת, פסטה ישנה או בננה בשלה שנשכחה? אל תזרקו! אריאל בן חמו הופכת את השאריות העייפות מאתמול למנות הטעימות של היום\n",
      "authors: ['  שפית אורחת: אריאל בן חמו', ' בריאלי']\n",
      "keywords: ['פסטה', 'אורז', 'מתכונים']\n",
      "**************************\n",
      "title: בני משפחת קוץ הובאו למנוחות: \"איך אפשר לקבור חמש נפשות טהורות?\"\n",
      "text:    חמשת בני משפחת קוץ - ההורים לבנת ואביב, וילדיהם רותם (19), יונתן (16) ויפתח (14) – הובאו למנוחות בצהריים (יום ג') זה לצד זה בבית העלמין בגן יבנה. אלפי בני אדם הגיעו ללוות בדרכם האחרונה את בני המשפחה שנרצחו במיטה בביתם בכפר עזה.                 יהודה לוי, אביה של לבנת, קרא קדיש. זיו, אחיה של לבנת, ספד: \"איך אפשר לקבור חמש נפשות טהורות? יפתח, איזה ילד חזק היית עם נפש כל כך עדינה ויפה. יונתן, אמרתי לך תמיד שעוד לא מאוחר להחליף קבוצה, עכשיו כבר מאוחר מדי. רותם, כל כך יפה ומוכשרת, תמיד הכניסה אור וצחוק לחדר. כמה היינו גאים בך כשסיימת קורס מ\"כים. אביב, היית כמו אח, אבא למופת. היית שם גם בשבת הארורה הזאת, כשניסית לכסות ולהגן על היקרים לך\". הוא הוסיף: \"לבנת, רדפת אחרי חלומות, בלעת בשלוקים גדולים את החיים. איך היית גאה במשפחה שלך, כמה השקעת בהם, כל פעם חשבת איך לתת, איך לתרום. היית לוחמת למען המשפחה שלך, למען הצדק. יש לי עכשיו חמש ציפורים קטנות בלב ואנחנו נהיה חזקים ונדאג להמשיך מה שהתחלתם\". עדי, אחותה של לבנת, אמרה: \"מנסה לאסוף מילה ועוד מילה, לחבר משפטים, לדמיין אתכם, להיזכר ברגעים המשותפים יחד, ברגעי השמחה והעצב. לשמוע את הצחוק המתגלגל שלכם. אבל החוט בין הראש ללב מנותק ולא מצליחה לעכל. לימדתם אותנו מהי משפחה שדואגת אחד לשני ועושה הכול אחד למען השני בכל מחיר ודרך. לבנת ואביב, גידלתם ילדים שחשבו אחרת, שהיו מלאי אהבה, שידעו שיש להם הורים איתנים ואהובים שיעשו הכול עבורם. הייתם אנשי חזון ואהבת הארץ. אני מבטיחה שנמשיך את המורשת לעוד שנים רבות\".     שרון, אחותו של אביב: \"אביבי שלי, אהוב לבי. היה לי את האח הכי טוב שיכולתי לבקש. לימדת אותי כל סודות הצילום, היית זה שכולם אהבו ורצו להיות בחברתו. ידעתי שאני יכולה לסמוך עליך בכל. דאגת לי תמיד ושמרת עליי. היית אבא כל כך אוהב ומסור. לא התפלאו כשמצאו אותך מחבק אותם ומנסה לשמור עליהם, כי אהבת אותם כל כך. לבנתי שלנו אהובה, תודה שלימדת אותי איך להתעקש ולהעיז, איך לפרוץ גבולות. תודה על הלב הרחב\". גם בת הדודה, אביב, ספדה: \"רותם, היית יותר מבת דודה, היית אחותי השנייה, הנפש שלי, מי שתמיד הייתי שמחה בחברתה. אני ריקה מתוכן, ריקה מרגשות, מנסה להיזכר בך, בקול שלך, בצחוק המתגלגל, במי שהיית ומה היית עושה במקומי. מנסה להיזכר בחוויות שלנו יחד וכמה הצחקנו את כולם. ואיך כל פעם שנכנסנו למקום חדש היו יודעים, הנה רותם ואביב. כמה הייתי משלמת בשביל עוד רגע אחרון איתך, או אפילו הקלטה או סרטון שלך צוחקת איתי על דברים שרק אנחנו היינו מבינות. תשמרי עלינו מלמעלה, לילה טוב רותם שלי\". בני, אביו של אביב, אמר: \"מאיפה מתחילים להספיד ולדבר על חמישה בני המשפחה היקרים ביותר? את מי לבכות קודם, איך אפשר? את הנערים התמירים, הדקלים הגבוהים הפורחים שיש להם אור בעיניים וטוב בנפש, את החבצלת שלנו, חכמה ושופעת אושר שנהנית מכל רגע בחיים, אולי נבכה על לבנת, אמא של שקט ואהבה, שוחרת שלום, צנועה וענווה. ולבי ונפשי מבכים את הילד הבכור שלי אביב, שכולו לב זהב ואני כבר כל כך מתגעגע אליו, אל המקסים הנאהב\". אלוף-משנה שלומי קדם, מפקד בסיס ההדרכה של חיל הקשר והתקשוב ואחד ממפקדיה של רותם, ספד: \"כמפקדת כיתת טירונות בבה\"ד, רותם מילאה את תפקידה באחריות רבה ויכולת התמודדת בלתי מתפשרת עם כל אתגר ומשימה. מי שהכיר אותה מספר שזכה ממנה לחברות אמת, מקצועיות בלתי מתפשרת, איתנות ומעל הכול חיוכה הגדול המשרה טוב ורוגע על כל סובביה. רותם ומשפחתה נלקחו בטרם עת בטבח חסר רחמים, הם היו מאוחדים כל כך בחייהם ובמותם. כולנו מרכינים ראש ומחבקים אתכם ומתחייבים להמשיך להפיץ את אורה של רותם\".\n",
      "link: https://www.ynet.co.il/news/article/rkm8db2w6#autoplay\n",
      "date_published: 2023-10-17T14:32:57.828762Z\n",
      "summary: אלפים ליוו בדרכם האחרונה את לבנת ואביב, וילדיהם רותם (19), יונתן (16) ויפתח (14) שנרצחו בביתם בכפר עזה ונטמנו זה לצד זה. \"לימדתם אותנו מהי משפחה שדואגת אחד לשני ועושה הכול אחד למען השני בכל מחיר ודרך\", ספדו בני המשפחה, \"נהיה חזקים, נמשיך את המורשת\"\n",
      "authors: [\"מאיר תורג'מן\"]\n",
      "keywords: ['הלוויה', 'טבח', 'חרבות ברזל', 'כפר עזה']\n",
      "**************************\n",
      "title: כוויות ושאיפת עשן: להציל חיים בדקות הראשונות | מדריך מד\"א מסביר\n",
      "text: כוויות או שאיפת עשן הפכו לצערנו לחלק מהמציאות בימי המלחמה. חשוב לדעת איך לטפל בצורה נכונה עד הגעתם של צוותי הרפואה. תומר מן, חובש מדריך במד\"א, התארח באולפן ynet Live והסביר כיצד יש לנהוג במקרים מסוג זה. פרק רביעי בסדרת מדריכי עזרה ראשונה מצולמים.    הכתבות הקודמות בסדרה: מדריך מד\"א מסביר: כך תניחו חסם עורקים - שלב אחר שלב | צפו כך תצליחו לבצע החייאה - ותוכלו להציל חיים | צפו נפילות בדרך לממ\"ד: כך תגישו עזרה ראשונה במקרה של פציעה | צפו \"הדבר הכי חשוב בכוויות זה כמובן דבר ראשון להוריד בגדים שעלולים להידבק כתוצאה מהחום\", אומר מן. \"כל מה שעוד לא נדבק יש להוריד. גם צמידים או טבעות. הדברים האלה יכולים ליצור המון לחץ לאחר מכן ואפילו חוסם עורקים\". ברגע שמישהו נכווה להפשיט אותו מהכול? \"כן, באזור של הכוויה. לא צריך את כל הגוף אם רק היד נכוותה, אבל הסכנה הכי גדולה שלנו בעצם בכוויות זה זיהום. בגלל זה הפתרון שלנו לאחר ההפשטה יהיה לשטוף כמה שאפשר את האזור במים זורמים פושרים במשך 10 דקות לפחות, או עד שהאמבולנס מגיע. אם האמבולנס לא מגיע יש להמשיך לשטוף את האזור\".   במקרים של כוויות גדולות ונרחבות על הגוף, הטיפול הוא מעט שונה. \"במקרים שבהם שליש מהגוף סובל מכוויות יש סכנה של היפותרמיה, של איבוד חום\", מתריע מן. \"זה מעט מוזר לאנשים כי הכוויה אמנם נגרמת מחום, אך כתוצאה ממנה אנו מאבדים את חום הגוף. במקרים האלה נרצה לכסות את הבן אדם כדי להשאיר את הגוף שלו כמה שיותר חם\". הנרצחים והחללים שקיפחו את חייהם במלחמה - אלו השמות אתם לא לבד: מוקדי סיוע, שיחות טיפוליות בחינם והקלות במתן תרופות במה ניתן לכסות? בשמיכה שמצאנו בבית? \"כן, שמיכה או כל דבר ששומר חום יכול לעבוד. זו הסיבה אגב שאנחנו שוטפים את אזור הכוויה במים פושרים ולא קרים. מצד אחד נרצה להוריד דברים שעלולים להידבק, אך מצד שני אם אנחנו מרגישים שהאדם נכנס למצב של היפותרמיה כן נרצה לעטוף אותו\".   לגבי שאיפת עשן, רוב הטיפול יתבצע בבית החולים, אך גם כאן ניתן לנקוט באמצעים מניעתיים. \"במקרים של שאיפת עשן, אדם צריך חמצן ובית חולים\", מסביר מן, \"אך אם אתם נמצאים בחדר סגור עם המון עשן, חשוב לזכור שעשן עולה למעלה, לכן חשוב להיצמד למטה כמה שיותר. אנחנו רוצים להיות כמה שאפשר במקום שבו אין עשן. זה הטיפול המניעתי הכי טוב שאפשר לעשות\".\n",
      "link: https://www.ynet.co.il/health/article/bjsfyt9b6#autoplay\n",
      "date_published: 2023-10-18T06:18:43.590302Z\n",
      "summary: שאיפת עשן, או כוויות כתוצאה מאש או חום הן פציעות נפוצות בימי מלחמה. כיצד תדעו להעניק עזרה באופן הנכון? תומר מן, חובש מדריך במד\"א, הסביר באולפן ynet Live כיצד יש לפעול אם אדם בקרבתכם נפגע. פרק נוסף בסדרת מדריכי עזרה ראשונה \n",
      "authors: ['ניר (שוקו) כהן', ' איתן גפן']\n",
      "keywords: ['כוויות', 'חרבות ברזל']\n",
      "**************************\n"
     ]
    }
   ],
   "source": [
    "for article in articles[:3]:\n",
    "    print(f\"title: {article['title']}\")\n",
    "    print(f\"text: {article['text']}\")\n",
    "    print(f\"link: {article['link']}\")\n",
    "    print(f\"date_published: {article['date_published']}\")\n",
    "    print(f\"summary: {article['summary']}\")\n",
    "    print(f\"authors: {article['authors']}\")\n",
    "    print(f\"keywords: {article['keywords']}\")\n",
    "    print(\"**************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we will build a list of Langchain [Document](https://docs.langchain.com/docs/components/schema/document) in order to use it later in the `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for article in articles:\n",
    "    doc =  Document(\n",
    "        page_content=article['text'], \n",
    "        metadata={\n",
    "            \"title\": article['title'], \n",
    "            \"link\": article['link'], \n",
    "            \"authors\": article['authors'],\n",
    "            \"date_published\": article['date_published'],\n",
    "            \"summary\": article['summary'],\n",
    "            \"keywords\": article['keywords']\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the length of each of the articles we fetched from Ynet news site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5103, 3096, 1979, 1186, 3035, 2454, 5754, 1565, 7411, 14783, 3188, 2730, 1845, 6624, 2442, 11731, 2464, 4266, 1856, 1148, 3083, 3791, 2877, 3803, 6623, 3211, 1087, 1393, 1520, 4495, 2459, 5746, 7920, 6440, 4136, 4560, 3086, 3999, 2347, 2459, 1720, 9148, 1915, 7868, 2032, 4427, 2460, 3195, 1487, 2735, 3006, 2288, 1035, 634, 2883, 3015, 4614, 3600, 2408, 2423, 2050, 3185, 4460, 8331, 1983, 1895, 9566, 4668, 2693, 1742, 7785, 7284, 1708, 2650, 5452, 2872, 1227, 2029, 2298, 2232, 757, 2430, 3916, 3998, 9551, 4401, 2076, 4608, 985, 3281, 3286, 6724, 1306, 3438, 5745, 9755, 45919, 5260, 3755, 4782, 6315, 1747, 1786, 3376, 2650, 4741, 2659, 2981, 1963, 819, 2489, 35708, 15994, 361, 2655, 4663, 2181, 3958, 8704, 1588, 1242, 4071, 3361, 9910, 1345\n"
     ]
    }
   ],
   "source": [
    "len_array = []\n",
    "for doc in docs:\n",
    "    len_array.append(str(len(doc.page_content)))\n",
    "print(', '.join(len_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved articles text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 8k tokens, which roughly translates to ~32000 characters. For the sake of this use-case we are creating chunks of roughly 2000 characters with an overlap of 200 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 125 documents loaded is 4415 characters.\n",
      "After the split we have 358 documents, which is more than the original 125.\n",
      "Average length among 358 documents (after split) is 1668 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(\n",
    "    documents\n",
    ")\n",
    "avg_char_count_pre = avg_doc_length(docs)\n",
    "avg_char_count_post = avg_doc_length(splits)\n",
    "print(f\"Average length among {len(docs)} documents loaded is {avg_char_count_pre} characters.\")\n",
    "print(f\"After the split we have {len(splits)} documents, which is more than the original {len(docs)}.\")\n",
    "print(\n",
    "    f\"Average length among {len(splits)} documents (after split) is {avg_char_count_post} characters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model Id : amazon.titan-embed-text-v1\n",
      "Sample embedding of a document chunk:  [-0.02064487  0.22352532 -0.1292298  ... -0.20077093  0.17227271\n",
      " -0.11000463]\n",
      "Size of the embedding:  (1536,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "modelId = bedrock_embeddings.model_id\n",
    "print(\"Embedding model Id :\", modelId)\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the similar pattern embeddings could be generated for the entire corpus and stored in a vector store.\n",
    "\n",
    "Firt of all we have to create a vector store. In this workshop we will use ***Amazon OpenSerach serverless.***\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sts::062083580489:assumed-role/AmazonSageMaker-ExecutionRole-20190829T190746/SageMaker'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step can take a minute to complete so be patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = 'ynet-hebrew-rag-bedrock'\n",
    "index_name = \"ynet-hebrew-rag-bedrock-index\"\n",
    "encryption_policy_name = \"ynet-hebrew-rag-bedrock-sp\"\n",
    "network_policy_name = \"ynet-hebrew-rag-bedrock-np\"\n",
    "access_policy_name = 'ynet-hebrew-rag-bedrock-ap'\n",
    "\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "security_policy = aoss_client.create_security_policy(\n",
    "    name = encryption_policy_name,\n",
    "    policy = json.dumps(\n",
    "        {\n",
    "            'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AWSOwnedKey': True\n",
    "        }),\n",
    "    type = 'encryption'\n",
    ")\n",
    "\n",
    "network_policy = aoss_client.create_security_policy(\n",
    "    name = network_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AllowFromPublic': True}\n",
    "        ]),\n",
    "    type = 'network'\n",
    ")\n",
    "\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "while True:\n",
    "    status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "    if status in ('ACTIVE', 'FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "access_policy = aoss_client.create_access_policy(\n",
    "    name = access_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'Resource': ['collection/' + vector_store_name],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateCollectionItems',\n",
    "                            'aoss:DeleteCollectionItems',\n",
    "                            'aoss:UpdateCollectionItems',\n",
    "                            'aoss:DescribeCollectionItems'],\n",
    "                        'ResourceType': 'collection'\n",
    "                    },\n",
    "                    {\n",
    "                        'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateIndex',\n",
    "                            'aoss:DeleteIndex',\n",
    "                            'aoss:UpdateIndex',\n",
    "                            'aoss:DescribeIndex',\n",
    "                            'aoss:ReadDocument',\n",
    "                            'aoss:WriteDocument'],\n",
    "                        'ResourceType': 'index'\n",
    "                    }],\n",
    "                'Principal': [identity],\n",
    "                'Description': 'Easy data policy'}\n",
    "        ]),\n",
    "    type = 'data'\n",
    ")\n",
    "\n",
    "host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com:443'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inject our documents into vector store. This can be easily done using [OpenSearch](https://python.langchain.com/docs/integrations/vectorstores/opensearch) implementation inside [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) which takes  input the embeddings model and the documents to create the entire vector store. Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation) helps us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, os.environ.get(\"AWS_DEFAULT_REGION\", None), service)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    splits,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Vector Store and Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We can use the similarity search method to make a query and return the chunks of text without any LLM generating the response.\n",
    "\n",
    "It takes a few seconds to make documents availible in index. **If you will get an empty output in a next cell, just wait a little bit and retry**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='תבנו גשרים, אבל לוקח עשר שנים עד שמשלמים את הכסף. הכל איטי מדי. האמריקאים רוצים לעזור לישראל, גם כלכלית וגם צבאית, אבל גם אצלם זה עובר במנגנונים זמן רב. עם זאת, כדאי לזכור שבסופו של דבר כל הכספים האלו ישטפו אותנו. זה עניין של זמן\". ״כדאי לזכור שבסופו של דבר כל הכספים האלו ישטפו אותנו. זה עניין של זמן״ אם למשברים יש דינמיקה דומה, למה לא ראינו עליות בשוק המניות? \"על שוק המניות פועלים שני נתונים: הצפי לגבי רווחי החברות וגובה הריבית. אנחנו נמצאים עם שני סימני שאלה גדולים מאוד בנוגע לשניהם. לגבי הרווחיות של החברות, היום אף אחד לא קונה, לא מבלה ולא טס. כל משבר והזמן שלו. אחרי היציאה מהמשבר יש ריבאונד. בקורונה זה לקח שנה. בשלב הראשון יש שיתוק. בהרבה חברות העובדים לא נמצאים והן לא עובדות בתפוקה מלאה. אבל למדנו שזה זמני ושאחרי אירועים יש פיצוי\". \"הרי הממשלות לא יעילות ומי ייחלץ בסוף לעזרה? בנק ישראל\", אומר כהנוביץ\\'. \"כרגע הנגיד עדיין זהיר, אבל יש כבר פעולות. בנק ישראל הודיע שהוא מוכר רזרבות מטבע חוץ ואמור לספק נזילות למוסדיים. הרי לא רחוק היום שנתחיל לשמוע טענות כמו \\'הריבית חונקת אותנו, אתה לא רוצה לעזור במאמץ המלחמתי?\\', או \\'המדינה צריכה לגייס כספים, תתחיל להוריד את הריבית\\'. האנשים עדיין בהלם והביקורת מעודנת ול כן אף אחד לא מתלהם. אבל מתישהו האזרחים ירגישו שאין להם כסף, בטווח הקצר או הארוך, והלחץ על בנק ישראל להוריד את הריבית יגדל\". אין חשש שהדולר יטפס לגבהים משמעותיים עם הורדת הריבית, לאחר שכבר חצה את המחסום של 4 שקלים? \"ברגע שחברות הביטוח יתחילו לקבל תביעות בעקבות הנזק שנגרם מהמלחמה – והרבה מהתביעות הללו משולמות בחו\"ל על ידי מבטחי משנה – יתחילו לזרום דולרים לארץ. זה קרה גם בפיצוץ הגרעיני ביפן. הין ביפן זינק באותה תקופה, וזה לא הובן מאחר שנגרם נזק לכלכלה. אבל הפיצויים של האסון הביאו לזרימת הרבה מטבע זר למדינה\". מה היית מציע למשקיעים? \"הרבה דברים הולכים לקרות פה מבחינה כלכלית, אבל זה אף פעם לא נכון לברוח מהשוק. צריך להגיע לאירועים כאלה עם תיק מאוזן. למזלנו רובנו מאוזנים. אין לשכוח שכל המוסדיים בישראל מלכתחילה מוטים לחו\"ל ולכן כלל תיקי ההשקעות פה, לנוכח הנזקים בכלכלה המקומית, די מאוזנים. בשנים האחרונות המוסדיים עשו הרבה הסטה לחו\"ל בגלל הבעיות בישראל ועכשיו, בשעת משבר,', metadata={'title': '\"כרגע כולם חוששים, אבל בסוף יהיה בארץ שיטפון של כספים\"', 'link': 'https://www.calcalist.co.il/market/article/h1uboqhbp?ref=ynet', 'authors': ['אלמוג עזר'], 'date_published': '2023-10-18T03:00:00.000000Z', 'summary': 'הכלכלן אמיר כהנוביץ\\' צופה כי המשק ייכנס למיתון בעקבות המלחמה ומעריך שלאחריו יהיה ריבאונד כלכלי: בנק ישראל ייאלץ להוריד את הריבית והאוצר יצטרך להזרים כספים כדי לעורר את הכלכלה. \"עכשיו יש קיפאון אבל ניסיון העבר מלמד שזה זמני\"', 'keywords': ['בנק ישראל', 'ריבית', 'פרופיט', 'פיחות', 'מלחמה בעזה', \"אמיר כהנוביץ'\"]}),\n",
       " Document(page_content='המלחמה בעזה שעלולה להימשך שבועות ארוכים והחשש מפני קיפאון כלכלי מביאים את שוק ההון לחזות כי אנחנו נמצאים לפני הקלות כלכליות. הצפי נכון לעכשיו הוא להקלות מוניטריות כמו הורדת ריבית והקלות פיסקאליות כמו חלוקת פיצויים לחברות על ידי משרד האוצר.  ההוצאות הללו עשויות שלא להספיק כדי למנוע מהמשק להיכנס למיתון אולם הן אמורות להביא גם לריבאונד כלכלי משמעותי לאחר מכן. כך לפחות סבור אמיר כהנוביץ\\', הכלכלן הראשי של פרופיט והכלכלן הראשי לשעבר של הפניקס. \"כשאני מסתכל על אירועים דומים, כלומר כאלו שדרשו התערבות של בנק ישראל ושל הממשלה, וכשאני מוסיף לכך שיהיה פה גם סיוע של הממשל האמריקאי ושל יהודי התפוצות, אני יודע שמה שמבשלים עכשיו יגיע בעוד כמה חודשים לכדי שיטפון\", הוא צופה. כהנוביץ\\' מסביר כי הדינמיקה של משברים דומה למדי: שוק ההון יסבול בזמן הקרוב אך עם היציאה מהמלחמה תיווצר נזילות פיננסית שתדחוף את השווקים. \"כרגע כולם חוששים ויש ירידות בשוק ההון. בשלב הראשון, הממשלה משותקת אבל הם בטירוף של להוציא ומהר. כולם רוצים להוציא כסף וכולם בקרוב יהיו פה בעד. בשלב השני נראה ערימות של כספים שנשפכים ומחלחלים לכל הכיוונים\", הוא אומר. כהנוביץ\\' לא מאמין בכך שפעולות הממשלה יוכלו לרסן באופן משמעותי את ההשלכות הכלכליות של המלחמה בעזה. לדבריו, \"גם אם תבוא הממשלה האוהדת ביותר, כלומר כזו שרוצה לשפוך כספים אדירים על המשק, יהיה לה קשה מאוד לשפוך כסף בצורה מהירה ויעילה. הרי כל החלטה צריכה לעבור מכרזים וועדות. אנחנו רק בשבוע השני של הלחימה ולכן קשה לקבוע מה יהיו אותם צעדים. אבל צריך להודות בכך שהממשלות לא יעילות. גם אם ירצו לעזור, לא יידעו למי להעביר כסף\". \"ראינו את זה קורה בקורונה\", הוא מציין. \"לאחר פרק זמן של תסכול מסוים, החליטו הממשלות בארה\"ב ובישראל שהן לא יודעות למי להעביר את הכסף כדי לעורר את הכלכלה ולכן חילקו לכולם אף שצעד כזה גורר הרבה ביקורת על הוצאות לא יעילות ולא מחושבות\". אפשרות נוספות של הממשלה היא להוציא בעצמה את המשק ממיתון על ידי עבודות יזומות, אולם על פי כהנוביץ\\' גם האפשרות הזו אינה מעשית: \"אומרים תבנו גשרים, אבל לוקח עשר שנים עד שמשלמים את הכסף. הכל איטי מדי. האמריקאים רוצים לעזור לישראל, גם כלכלית וגם צבאית, אבל גם אצלם זה עובר במנגנונים זמן רב. עם זאת, כדאי לזכור שבסופו של דבר כל הכספים', metadata={'title': '\"כרגע כולם חוששים, אבל בסוף יהיה בארץ שיטפון של כספים\"', 'link': 'https://www.calcalist.co.il/market/article/h1uboqhbp?ref=ynet', 'authors': ['אלמוג עזר'], 'date_published': '2023-10-18T03:00:00.000000Z', 'summary': 'הכלכלן אמיר כהנוביץ\\' צופה כי המשק ייכנס למיתון בעקבות המלחמה ומעריך שלאחריו יהיה ריבאונד כלכלי: בנק ישראל ייאלץ להוריד את הריבית והאוצר יצטרך להזרים כספים כדי לעורר את הכלכלה. \"עכשיו יש קיפאון אבל ניסיון העבר מלמד שזה זמני\"', 'keywords': ['בנק ישראל', 'ריבית', 'פרופיט', 'פיחות', 'מלחמה בעזה', \"אמיר כהנוביץ'\"]}),\n",
       " Document(page_content='שיעור האבטלה בישראל המשיך לרדת גם בחודש ספטמבר לשיעור של 3.4%, אולם נראה שזה יהיה השיא הנמוך האחרון לתקופה הקרובה. כעת חוזים במשק עליה חדה במיוחד בשיעור האבטלה, בשל הפגיעה הקשה בעסקים ובחברות עקב מלחמת \"חרבות ברזל\", שפרצה כבר בשבוע הראשון של חודש אוקטובר. האבטלה בישראל היא כיום עדיין מהנמוכות בעולם, אך צפויה להיות שוב גבוהה בימי המלחמה.     לפי שעה אין עדיין הערכות ביחס לשיעור הגידול, אולם ידוע כבר על עשרות אלפי עובדים שנשלחו לחופשת בבית, בין השאר מענפי התיירות, הבידור והמסעדנות, ולפי שעה אין כוונה במשרדי האוצר ובביטוח הלאומי לחזור לנוסחת החל\"ת מימי משבר הקורונה. בביטוח הלאומי מסבירים כי העובדים יפסידו חלק ניכר ממשכורתם אם תאומץ שיטת החל\"ת מהקורונה, מה עוד שעובדים רבים טרם צברו את תקופת האכשרה של 12 חודשי עבודה הדרושים מתוך 18 החודשים האחרונים כדי לקבל עד 70% דמי אבטלה (דהיינו שנת עבודה דרושה מאז אפריל 2022). מנתוני הלמ\"ס שפורסמו היום עולה כי בספטמבר היו בישראל  151.1 אלף מובטלים, שהם 3.4% מהעובדים והמבקשים לעבוד במשק. זאת, לעומת 155.9 אלף מובטלים, שהיו 3.5% באוגוסט השנה. על פי הנתונים יחד עם המועסקים שנעדרו זמנית מעבודתם היו בספטמבר 163.6 בלתי מועסקים, שהם 3.6%, בהשוואה ל-171.0 מובטלים, שהם 3.8% באוגוסט. בצירוף עובדים שפוטרו בשל סגירת מקום העבודה בשנתיים האחרונות (עדיין בעת משבר הקורונה) שיעור האבטלה בספטמבר הוא 4.0%, שהם 181.5 אלף, לעומת 186.0 אלף, שהם 4.1% באוגוסט. יחד עם מי שכבר התייאשו מלמצוא עבודה הגיע שיעור האבטלה המורחב ביותר בספטמבר ל-4.2%, שהם 191.5 אלף, לעומת באוגוסט 4.5% מובטלים, שהם 205.3 אלף ישראלים, דהיינו עדיין מעל מאתיים אלף מובטלים. בסך הכל היו בישראל בספטמבר 7,085,300 בני 15 ומעלה שיכולים היו לעבוד, לפי ההגדרות הבינלאומיות. מתוכם היו בכוח העבודה 4,493,800 ישראלים, שהם 63.4%, לעומת 2,591,500 נשים וגברים שלא נכללו בכוח העבודה, כיוון שלא עבדו ולא חיפשו כלל עבודה בחודש ספטמבר.', metadata={'title': 'לפני הזינוק הצפוי בשל המלחמה: שיעור האבטלה היה נמוך מאוד גם בספטמבר', 'link': 'https://www.ynet.co.il/economy/article/0gbm7uxt8', 'authors': ['גד ליאור'], 'date_published': '2023-10-18T09:59:28.260556Z', 'summary': 'לפי נתוני הלמ\"ס, שיעור האבטלה עמד בחודש האחרון לפני פרוץ המלחמה על 3.4% בלבד. בשלב זה אין עדיין הערכות ביחס לשיעור הגידול הצפוי, אולם ידוע כבר על עשרות אלפי עובדים שנשלחו לחופשת בבית. לפי שעה אין כוונה במשרדי האוצר ובביטוח הלאומי לחזור לנוסחת החל\"ת מימי הקורונה', 'keywords': ['אבטלה', 'למס']})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"מה מצב הכלכלה?\"\n",
    "\n",
    "results = docsearch.similarity_search(query, k=3)  # our search query  # return 3 most relevant docs\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All of these are relevant results, telling us that the retrieval component of our systems is functioning. The next step is adding our LLM to generatively answer our question using the information provided in these retrieved contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Question Answering\n",
    "\n",
    "In generative question-answering (GQA), we pass our question to the Claude-2 but instruct it to base the answer on the information returned from our knowledge base. We can do this in LangChain easily using the RetrievalQA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s try this with our earlier query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' לפי הטקסט, מצב הכלכלה בישראל צפוי להיפגע בחודשים הקרובים בעקבות מלחמת חרבות ברזל: \\n\\n- שיעור האבטלה, שהיה נמוך מאוד (3.4%) בחודש ספטמבר, צפוי לעלות בחדות בשל פיטורים המוניים בענפי התיירות, הבידור והמסעדנות.\\n\\n- השקל התחזק מול הדולר והאירו, והמדדים המובילים בבורסה ירדו במעט. \\n\\n- חברת הדירוג פיץ\\' הציבה את דירוג האשראי של ישראל תחת \"מעקב שלילי\" בשל הסיכון הגאופוליטי.\\n\\n- מומחים צופים יציאת משקיעים זרים מהשקעות בישראל, מה שיתרום לפיחות נוסף של השקל.\\n\\n- בנק ישראל מנסה לייצב את המצב על ידי מכירת מט\"ח, אך נראה שלא יוריד את הריבית בזמן הקרוב.\\n\\nכך שבסך הכל המצב הכלכלי בישראל צפוי להידרדר בחודשים הקרובים, אך קשה לאמוד בדיוק עד כמה.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re still not entirely protected from convincing yet false hallucinations by the model, they can happen, and it’s unlikely that we can eliminate the problem completely. However, we can do more to improve our trust in the answers provided.\n",
    "\n",
    "An effective way of doing this is by adding citations to the response, allowing a user to see where the information is coming from. We can do this by adding a parameter: `return_source_documents=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_with_sources = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={'k': 3}),return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'מה מצב הכלכלה?',\n",
       " 'result': ' על פי הדיווחים נראה שמצב הכלכלה בישראל עומד להיפגע באופן משמעותי בעקבות מבצע \"חרבות ברזל\" בעזה, שהחל בתחילת אוקטובר 2022. \\n\\nשיעור האבטלה בישראל אמנם הגיע בספטמבר לשפל של 3.4%, אבל צפויה עלייה חדה באבטלה בשל המלחמה. כבר עתה ידוע על עשרות אלפי עובדים שנשלחו לחל\"ת בענפי התיירות, הבידור והמסעדנות. \\n\\nהתחזית היא ששיעור האבטלה יעלה באופן משמעותי, אך טרם ידוע במדויק בכמה. לא ברור גם האם יאומץ שוב מנגנון החל\"ת מימי הקורונה או לא.\\n\\nנראה אם כן שהמלחמה תפגע קשות בכלכלה הישראלית ותגרום לעלייה חדה באבטלה לאחר תקופה ארוכה של ירידה בשיעור המובטלים. עדיין מוקדם להעריך במדויק עד כמה, אך ברור שמדובר בפגיעה משמעותית.',\n",
       " 'source_documents': [Document(page_content='תבנו גשרים, אבל לוקח עשר שנים עד שמשלמים את הכסף. הכל איטי מדי. האמריקאים רוצים לעזור לישראל, גם כלכלית וגם צבאית, אבל גם אצלם זה עובר במנגנונים זמן רב. עם זאת, כדאי לזכור שבסופו של דבר כל הכספים האלו ישטפו אותנו. זה עניין של זמן\". ״כדאי לזכור שבסופו של דבר כל הכספים האלו ישטפו אותנו. זה עניין של זמן״ אם למשברים יש דינמיקה דומה, למה לא ראינו עליות בשוק המניות? \"על שוק המניות פועלים שני נתונים: הצפי לגבי רווחי החברות וגובה הריבית. אנחנו נמצאים עם שני סימני שאלה גדולים מאוד בנוגע לשניהם. לגבי הרווחיות של החברות, היום אף אחד לא קונה, לא מבלה ולא טס. כל משבר והזמן שלו. אחרי היציאה מהמשבר יש ריבאונד. בקורונה זה לקח שנה. בשלב הראשון יש שיתוק. בהרבה חברות העובדים לא נמצאים והן לא עובדות בתפוקה מלאה. אבל למדנו שזה זמני ושאחרי אירועים יש פיצוי\". \"הרי הממשלות לא יעילות ומי ייחלץ בסוף לעזרה? בנק ישראל\", אומר כהנוביץ\\'. \"כרגע הנגיד עדיין זהיר, אבל יש כבר פעולות. בנק ישראל הודיע שהוא מוכר רזרבות מטבע חוץ ואמור לספק נזילות למוסדיים. הרי לא רחוק היום שנתחיל לשמוע טענות כמו \\'הריבית חונקת אותנו, אתה לא רוצה לעזור במאמץ המלחמתי?\\', או \\'המדינה צריכה לגייס כספים, תתחיל להוריד את הריבית\\'. האנשים עדיין בהלם והביקורת מעודנת ול כן אף אחד לא מתלהם. אבל מתישהו האזרחים ירגישו שאין להם כסף, בטווח הקצר או הארוך, והלחץ על בנק ישראל להוריד את הריבית יגדל\". אין חשש שהדולר יטפס לגבהים משמעותיים עם הורדת הריבית, לאחר שכבר חצה את המחסום של 4 שקלים? \"ברגע שחברות הביטוח יתחילו לקבל תביעות בעקבות הנזק שנגרם מהמלחמה – והרבה מהתביעות הללו משולמות בחו\"ל על ידי מבטחי משנה – יתחילו לזרום דולרים לארץ. זה קרה גם בפיצוץ הגרעיני ביפן. הין ביפן זינק באותה תקופה, וזה לא הובן מאחר שנגרם נזק לכלכלה. אבל הפיצויים של האסון הביאו לזרימת הרבה מטבע זר למדינה\". מה היית מציע למשקיעים? \"הרבה דברים הולכים לקרות פה מבחינה כלכלית, אבל זה אף פעם לא נכון לברוח מהשוק. צריך להגיע לאירועים כאלה עם תיק מאוזן. למזלנו רובנו מאוזנים. אין לשכוח שכל המוסדיים בישראל מלכתחילה מוטים לחו\"ל ולכן כלל תיקי ההשקעות פה, לנוכח הנזקים בכלכלה המקומית, די מאוזנים. בשנים האחרונות המוסדיים עשו הרבה הסטה לחו\"ל בגלל הבעיות בישראל ועכשיו, בשעת משבר,', metadata={'title': '\"כרגע כולם חוששים, אבל בסוף יהיה בארץ שיטפון של כספים\"', 'link': 'https://www.calcalist.co.il/market/article/h1uboqhbp?ref=ynet', 'authors': ['אלמוג עזר'], 'date_published': '2023-10-18T03:00:00.000000Z', 'summary': 'הכלכלן אמיר כהנוביץ\\' צופה כי המשק ייכנס למיתון בעקבות המלחמה ומעריך שלאחריו יהיה ריבאונד כלכלי: בנק ישראל ייאלץ להוריד את הריבית והאוצר יצטרך להזרים כספים כדי לעורר את הכלכלה. \"עכשיו יש קיפאון אבל ניסיון העבר מלמד שזה זמני\"', 'keywords': ['בנק ישראל', 'ריבית', 'פרופיט', 'פיחות', 'מלחמה בעזה', \"אמיר כהנוביץ'\"]}),\n",
       "  Document(page_content='המלחמה בעזה שעלולה להימשך שבועות ארוכים והחשש מפני קיפאון כלכלי מביאים את שוק ההון לחזות כי אנחנו נמצאים לפני הקלות כלכליות. הצפי נכון לעכשיו הוא להקלות מוניטריות כמו הורדת ריבית והקלות פיסקאליות כמו חלוקת פיצויים לחברות על ידי משרד האוצר.  ההוצאות הללו עשויות שלא להספיק כדי למנוע מהמשק להיכנס למיתון אולם הן אמורות להביא גם לריבאונד כלכלי משמעותי לאחר מכן. כך לפחות סבור אמיר כהנוביץ\\', הכלכלן הראשי של פרופיט והכלכלן הראשי לשעבר של הפניקס. \"כשאני מסתכל על אירועים דומים, כלומר כאלו שדרשו התערבות של בנק ישראל ושל הממשלה, וכשאני מוסיף לכך שיהיה פה גם סיוע של הממשל האמריקאי ושל יהודי התפוצות, אני יודע שמה שמבשלים עכשיו יגיע בעוד כמה חודשים לכדי שיטפון\", הוא צופה. כהנוביץ\\' מסביר כי הדינמיקה של משברים דומה למדי: שוק ההון יסבול בזמן הקרוב אך עם היציאה מהמלחמה תיווצר נזילות פיננסית שתדחוף את השווקים. \"כרגע כולם חוששים ויש ירידות בשוק ההון. בשלב הראשון, הממשלה משותקת אבל הם בטירוף של להוציא ומהר. כולם רוצים להוציא כסף וכולם בקרוב יהיו פה בעד. בשלב השני נראה ערימות של כספים שנשפכים ומחלחלים לכל הכיוונים\", הוא אומר. כהנוביץ\\' לא מאמין בכך שפעולות הממשלה יוכלו לרסן באופן משמעותי את ההשלכות הכלכליות של המלחמה בעזה. לדבריו, \"גם אם תבוא הממשלה האוהדת ביותר, כלומר כזו שרוצה לשפוך כספים אדירים על המשק, יהיה לה קשה מאוד לשפוך כסף בצורה מהירה ויעילה. הרי כל החלטה צריכה לעבור מכרזים וועדות. אנחנו רק בשבוע השני של הלחימה ולכן קשה לקבוע מה יהיו אותם צעדים. אבל צריך להודות בכך שהממשלות לא יעילות. גם אם ירצו לעזור, לא יידעו למי להעביר כסף\". \"ראינו את זה קורה בקורונה\", הוא מציין. \"לאחר פרק זמן של תסכול מסוים, החליטו הממשלות בארה\"ב ובישראל שהן לא יודעות למי להעביר את הכסף כדי לעורר את הכלכלה ולכן חילקו לכולם אף שצעד כזה גורר הרבה ביקורת על הוצאות לא יעילות ולא מחושבות\". אפשרות נוספות של הממשלה היא להוציא בעצמה את המשק ממיתון על ידי עבודות יזומות, אולם על פי כהנוביץ\\' גם האפשרות הזו אינה מעשית: \"אומרים תבנו גשרים, אבל לוקח עשר שנים עד שמשלמים את הכסף. הכל איטי מדי. האמריקאים רוצים לעזור לישראל, גם כלכלית וגם צבאית, אבל גם אצלם זה עובר במנגנונים זמן רב. עם זאת, כדאי לזכור שבסופו של דבר כל הכספים', metadata={'title': '\"כרגע כולם חוששים, אבל בסוף יהיה בארץ שיטפון של כספים\"', 'link': 'https://www.calcalist.co.il/market/article/h1uboqhbp?ref=ynet', 'authors': ['אלמוג עזר'], 'date_published': '2023-10-18T03:00:00.000000Z', 'summary': 'הכלכלן אמיר כהנוביץ\\' צופה כי המשק ייכנס למיתון בעקבות המלחמה ומעריך שלאחריו יהיה ריבאונד כלכלי: בנק ישראל ייאלץ להוריד את הריבית והאוצר יצטרך להזרים כספים כדי לעורר את הכלכלה. \"עכשיו יש קיפאון אבל ניסיון העבר מלמד שזה זמני\"', 'keywords': ['בנק ישראל', 'ריבית', 'פרופיט', 'פיחות', 'מלחמה בעזה', \"אמיר כהנוביץ'\"]}),\n",
       "  Document(page_content='שיעור האבטלה בישראל המשיך לרדת גם בחודש ספטמבר לשיעור של 3.4%, אולם נראה שזה יהיה השיא הנמוך האחרון לתקופה הקרובה. כעת חוזים במשק עליה חדה במיוחד בשיעור האבטלה, בשל הפגיעה הקשה בעסקים ובחברות עקב מלחמת \"חרבות ברזל\", שפרצה כבר בשבוע הראשון של חודש אוקטובר. האבטלה בישראל היא כיום עדיין מהנמוכות בעולם, אך צפויה להיות שוב גבוהה בימי המלחמה.     לפי שעה אין עדיין הערכות ביחס לשיעור הגידול, אולם ידוע כבר על עשרות אלפי עובדים שנשלחו לחופשת בבית, בין השאר מענפי התיירות, הבידור והמסעדנות, ולפי שעה אין כוונה במשרדי האוצר ובביטוח הלאומי לחזור לנוסחת החל\"ת מימי משבר הקורונה. בביטוח הלאומי מסבירים כי העובדים יפסידו חלק ניכר ממשכורתם אם תאומץ שיטת החל\"ת מהקורונה, מה עוד שעובדים רבים טרם צברו את תקופת האכשרה של 12 חודשי עבודה הדרושים מתוך 18 החודשים האחרונים כדי לקבל עד 70% דמי אבטלה (דהיינו שנת עבודה דרושה מאז אפריל 2022). מנתוני הלמ\"ס שפורסמו היום עולה כי בספטמבר היו בישראל  151.1 אלף מובטלים, שהם 3.4% מהעובדים והמבקשים לעבוד במשק. זאת, לעומת 155.9 אלף מובטלים, שהיו 3.5% באוגוסט השנה. על פי הנתונים יחד עם המועסקים שנעדרו זמנית מעבודתם היו בספטמבר 163.6 בלתי מועסקים, שהם 3.6%, בהשוואה ל-171.0 מובטלים, שהם 3.8% באוגוסט. בצירוף עובדים שפוטרו בשל סגירת מקום העבודה בשנתיים האחרונות (עדיין בעת משבר הקורונה) שיעור האבטלה בספטמבר הוא 4.0%, שהם 181.5 אלף, לעומת 186.0 אלף, שהם 4.1% באוגוסט. יחד עם מי שכבר התייאשו מלמצוא עבודה הגיע שיעור האבטלה המורחב ביותר בספטמבר ל-4.2%, שהם 191.5 אלף, לעומת באוגוסט 4.5% מובטלים, שהם 205.3 אלף ישראלים, דהיינו עדיין מעל מאתיים אלף מובטלים. בסך הכל היו בישראל בספטמבר 7,085,300 בני 15 ומעלה שיכולים היו לעבוד, לפי ההגדרות הבינלאומיות. מתוכם היו בכוח העבודה 4,493,800 ישראלים, שהם 63.4%, לעומת 2,591,500 נשים וגברים שלא נכללו בכוח העבודה, כיוון שלא עבדו ולא חיפשו כלל עבודה בחודש ספטמבר.', metadata={'title': 'לפני הזינוק הצפוי בשל המלחמה: שיעור האבטלה היה נמוך מאוד גם בספטמבר', 'link': 'https://www.ynet.co.il/economy/article/0gbm7uxt8', 'authors': ['גד ליאור'], 'date_published': '2023-10-18T09:59:28.260556Z', 'summary': 'לפי נתוני הלמ\"ס, שיעור האבטלה עמד בחודש האחרון לפני פרוץ המלחמה על 3.4% בלבד. בשלב זה אין עדיין הערכות ביחס לשיעור הגידול הצפוי, אולם ידוע כבר על עשרות אלפי עובדים שנשלחו לחופשת בבית. לפי שעה אין כוונה במשרדי האוצר ובביטוח הלאומי לחזור לנוסחת החל\"ת מימי הקורונה', 'keywords': ['אבטלה', 'למס']})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have answered the question being asked but also included the source of this information being used by the LLM.\n",
    "\n",
    "#### We’ve learned how to ground Large Language Models with source knowledge by using a vector database as our knowledge base. Using this, we can encourage accuracy in our LLM’s responses, keep source knowledge up to date, and improve trust in our system by providing citations with every answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this embedding of the query to then fetch relevant documents.\n",
    "Now our query is represented as embeddings we can do a similarity search of our query against our data store providing us with the most relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customisable option\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the helpf of [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) which can be specific to the model.\n",
    "\n",
    "Note: In this example we are using Anthropic Claude as the LLM under Amazon Bedrock, this particular model performs best if the inputs are provided under `Human:` and the model is requested to generate an output after `Assistant:`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " לסיכום המצב הכלכלי בישראל נכון לאוקטובר 2023:\n",
      "\n",
      "- שיעור האבטלה בישראל ירד ל-3.4% בספטמבר, אך צפוי לעלות שוב בגלל מלחמת עזה. כבר ידוע על עשרות אלפי עובדים שנשלחו לחל\"ת, בעיקר בתיירות ומסעדנות. \n",
      "\n",
      "- השקל המשיך להיחלש מול הדולר ונסחר סביב 4 ש\"ח לדולר, עלייה של 0.4% ביום. המדדים בבורסה ירדו ב-0.1%.\n",
      "\n",
      "- בנק ישראל רמז שלא בהכרח יוריד ריבית בקרוב, כדי לייצב את המטבע. \n",
      "\n",
      "- חברת הדירוג פיץ' הציבה את ישראל תחת \"מעקב שלילי\" בגלל הסיכון הגיאופוליטי, אך זה עדיין לא הורדת דירוג.\n",
      "\n",
      "- המומחים צופים צעדי הרחבה מוניטרית ופיסקלית מהממשלה ובנק ישראל שיביאו להתאוששות כלכלית לאחר המלחמה.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: אתה עיתונאי עוזר ומיידע. תסכם בשלוש-ארבע נקודות על פי המידע העדכני ביותר, מחודש אוקובר 2023. אם אתה לא יודע, תכתוב איני יודע.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_prompt = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "result = qa_prompt({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually search an article in OpenSearch Serverless\n",
    "\n",
    "Sometimes you need more flexibility than Lanchain can provide. In those cases you can use low level `boto3` and `opensearch-py` APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the mappings of our index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We 1st initialize an open search serverless client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "open_search_serverless_client = OpenSearch(\n",
    "        hosts=host,\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=300\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ynet-hebrew-rag-bedrock-index\": {\n",
      "    \"mappings\": {\n",
      "      \"properties\": {\n",
      "        \"id\": {\n",
      "          \"type\": \"text\",\n",
      "          \"fields\": {\n",
      "            \"keyword\": {\n",
      "              \"type\": \"keyword\",\n",
      "              \"ignore_above\": 256\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"metadata\": {\n",
      "          \"properties\": {\n",
      "            \"authors\": {\n",
      "              \"type\": \"text\",\n",
      "              \"fields\": {\n",
      "                \"keyword\": {\n",
      "                  \"type\": \"keyword\",\n",
      "                  \"ignore_above\": 256\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"date_published\": {\n",
      "              \"type\": \"date\"\n",
      "            },\n",
      "            \"keywords\": {\n",
      "              \"type\": \"text\",\n",
      "              \"fields\": {\n",
      "                \"keyword\": {\n",
      "                  \"type\": \"keyword\",\n",
      "                  \"ignore_above\": 256\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"link\": {\n",
      "              \"type\": \"text\",\n",
      "              \"fields\": {\n",
      "                \"keyword\": {\n",
      "                  \"type\": \"keyword\",\n",
      "                  \"ignore_above\": 256\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"summary\": {\n",
      "              \"type\": \"text\",\n",
      "              \"fields\": {\n",
      "                \"keyword\": {\n",
      "                  \"type\": \"keyword\",\n",
      "                  \"ignore_above\": 256\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"title\": {\n",
      "              \"type\": \"text\",\n",
      "              \"fields\": {\n",
      "                \"keyword\": {\n",
      "                  \"type\": \"keyword\",\n",
      "                  \"ignore_above\": 256\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"text\": {\n",
      "          \"type\": \"text\",\n",
      "          \"fields\": {\n",
      "            \"keyword\": {\n",
      "              \"type\": \"keyword\",\n",
      "              \"ignore_above\": 256\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"vector_field\": {\n",
      "          \"type\": \"knn_vector\",\n",
      "          \"dimension\": 1536,\n",
      "          \"method\": {\n",
      "            \"engine\": \"faiss\",\n",
      "            \"space_type\": \"l2\",\n",
      "            \"name\": \"hnsw\",\n",
      "            \"parameters\": {\n",
      "              \"ef_construction\": 512,\n",
      "              \"m\": 16\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "index_data = open_search_serverless_client.indices.get_mapping(index=index_name)\n",
    "print(dumps(index_data, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Query the index with KNN using embeddings from Bedrock Titan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'מה מצב הכלכלה?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create embeddings for the query string using Titan embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.640625,\n",
       " 0.484375,\n",
       " -0.32226562,\n",
       " -0.6171875,\n",
       " 0.043701172,\n",
       " 0.03857422,\n",
       " 0.18554688,\n",
       " -0.0008773804,\n",
       " 0.51171875,\n",
       " 0.22070312]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = boto3_bedrock.invoke_model(\n",
    "                body=json.dumps({\"inputText\": query}),\n",
    "                modelId=embed_model_id,\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "result = json.loads(response['body'].read())\n",
    "embedded_search = result.get('embedding')\n",
    "embedded_search[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_query = {\n",
    "                \"size\": 5,\n",
    "                \"query\": {\"knn\": {\"vector_field\": {\"vector\": embedded_search, \"k\": 2}}},\n",
    "                \"_source\": False,\n",
    "                \"fields\": [\"text\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'ynet-hebrew-rag-bedrock-index',\n",
       "  '_id': '1%3A0%3AaocYQ4sB9u5z628DJQir',\n",
       "  '_score': 0.0040803854,\n",
       "  'fields': {'text': ['תבנו גשרים, אבל לוקח עשר שנים עד שמשלמים את הכסף. הכל איטי מדי. האמריקאים רוצים לעזור לישראל, גם כלכלית וגם צבאית, אבל גם אצלם זה עובר במנגנונים זמן רב. עם זאת, כדאי לזכור שבסופו של דבר כל הכספים האלו ישטפו אותנו. זה עניין של זמן\". ״כדאי לזכור שבסופו של דבר כל הכספים האלו ישטפו אותנו. זה עניין של זמן״ אם למשברים יש דינמיקה דומה, למה לא ראינו עליות בשוק המניות? \"על שוק המניות פועלים שני נתונים: הצפי לגבי רווחי החברות וגובה הריבית. אנחנו נמצאים עם שני סימני שאלה גדולים מאוד בנוגע לשניהם. לגבי הרווחיות של החברות, היום אף אחד לא קונה, לא מבלה ולא טס. כל משבר והזמן שלו. אחרי היציאה מהמשבר יש ריבאונד. בקורונה זה לקח שנה. בשלב הראשון יש שיתוק. בהרבה חברות העובדים לא נמצאים והן לא עובדות בתפוקה מלאה. אבל למדנו שזה זמני ושאחרי אירועים יש פיצוי\". \"הרי הממשלות לא יעילות ומי ייחלץ בסוף לעזרה? בנק ישראל\", אומר כהנוביץ\\'. \"כרגע הנגיד עדיין זהיר, אבל יש כבר פעולות. בנק ישראל הודיע שהוא מוכר רזרבות מטבע חוץ ואמור לספק נזילות למוסדיים. הרי לא רחוק היום שנתחיל לשמוע טענות כמו \\'הריבית חונקת אותנו, אתה לא רוצה לעזור במאמץ המלחמתי?\\', או \\'המדינה צריכה לגייס כספים, תתחיל להוריד את הריבית\\'. האנשים עדיין בהלם והביקורת מעודנת ול כן אף אחד לא מתלהם. אבל מתישהו האזרחים ירגישו שאין להם כסף, בטווח הקצר או הארוך, והלחץ על בנק ישראל להוריד את הריבית יגדל\". אין חשש שהדולר יטפס לגבהים משמעותיים עם הורדת הריבית, לאחר שכבר חצה את המחסום של 4 שקלים? \"ברגע שחברות הביטוח יתחילו לקבל תביעות בעקבות הנזק שנגרם מהמלחמה – והרבה מהתביעות הללו משולמות בחו\"ל על ידי מבטחי משנה – יתחילו לזרום דולרים לארץ. זה קרה גם בפיצוץ הגרעיני ביפן. הין ביפן זינק באותה תקופה, וזה לא הובן מאחר שנגרם נזק לכלכלה. אבל הפיצויים של האסון הביאו לזרימת הרבה מטבע זר למדינה\". מה היית מציע למשקיעים? \"הרבה דברים הולכים לקרות פה מבחינה כלכלית, אבל זה אף פעם לא נכון לברוח מהשוק. צריך להגיע לאירועים כאלה עם תיק מאוזן. למזלנו רובנו מאוזנים. אין לשכוח שכל המוסדיים בישראל מלכתחילה מוטים לחו\"ל ולכן כלל תיקי ההשקעות פה, לנוכח הנזקים בכלכלה המקומית, די מאוזנים. בשנים האחרונות המוסדיים עשו הרבה הסטה לחו\"ל בגלל הבעיות בישראל ועכשיו, בשעת משבר,']}},\n",
       " {'_index': 'ynet-hebrew-rag-bedrock-index',\n",
       "  '_id': '1%3A0%3AaYcYQ4sB9u5z628DJQir',\n",
       "  '_score': 0.00403281,\n",
       "  'fields': {'text': ['המלחמה בעזה שעלולה להימשך שבועות ארוכים והחשש מפני קיפאון כלכלי מביאים את שוק ההון לחזות כי אנחנו נמצאים לפני הקלות כלכליות. הצפי נכון לעכשיו הוא להקלות מוניטריות כמו הורדת ריבית והקלות פיסקאליות כמו חלוקת פיצויים לחברות על ידי משרד האוצר.  ההוצאות הללו עשויות שלא להספיק כדי למנוע מהמשק להיכנס למיתון אולם הן אמורות להביא גם לריבאונד כלכלי משמעותי לאחר מכן. כך לפחות סבור אמיר כהנוביץ\\', הכלכלן הראשי של פרופיט והכלכלן הראשי לשעבר של הפניקס. \"כשאני מסתכל על אירועים דומים, כלומר כאלו שדרשו התערבות של בנק ישראל ושל הממשלה, וכשאני מוסיף לכך שיהיה פה גם סיוע של הממשל האמריקאי ושל יהודי התפוצות, אני יודע שמה שמבשלים עכשיו יגיע בעוד כמה חודשים לכדי שיטפון\", הוא צופה. כהנוביץ\\' מסביר כי הדינמיקה של משברים דומה למדי: שוק ההון יסבול בזמן הקרוב אך עם היציאה מהמלחמה תיווצר נזילות פיננסית שתדחוף את השווקים. \"כרגע כולם חוששים ויש ירידות בשוק ההון. בשלב הראשון, הממשלה משותקת אבל הם בטירוף של להוציא ומהר. כולם רוצים להוציא כסף וכולם בקרוב יהיו פה בעד. בשלב השני נראה ערימות של כספים שנשפכים ומחלחלים לכל הכיוונים\", הוא אומר. כהנוביץ\\' לא מאמין בכך שפעולות הממשלה יוכלו לרסן באופן משמעותי את ההשלכות הכלכליות של המלחמה בעזה. לדבריו, \"גם אם תבוא הממשלה האוהדת ביותר, כלומר כזו שרוצה לשפוך כספים אדירים על המשק, יהיה לה קשה מאוד לשפוך כסף בצורה מהירה ויעילה. הרי כל החלטה צריכה לעבור מכרזים וועדות. אנחנו רק בשבוע השני של הלחימה ולכן קשה לקבוע מה יהיו אותם צעדים. אבל צריך להודות בכך שהממשלות לא יעילות. גם אם ירצו לעזור, לא יידעו למי להעביר כסף\". \"ראינו את זה קורה בקורונה\", הוא מציין. \"לאחר פרק זמן של תסכול מסוים, החליטו הממשלות בארה\"ב ובישראל שהן לא יודעות למי להעביר את הכסף כדי לעורר את הכלכלה ולכן חילקו לכולם אף שצעד כזה גורר הרבה ביקורת על הוצאות לא יעילות ולא מחושבות\". אפשרות נוספות של הממשלה היא להוציא בעצמה את המשק ממיתון על ידי עבודות יזומות, אולם על פי כהנוביץ\\' גם האפשרות הזו אינה מעשית: \"אומרים תבנו גשרים, אבל לוקח עשר שנים עד שמשלמים את הכסף. הכל איטי מדי. האמריקאים רוצים לעזור לישראל, גם כלכלית וגם צבאית, אבל גם אצלם זה עובר במנגנונים זמן רב. עם זאת, כדאי לזכור שבסופו של דבר כל הכספים']}},\n",
       " {'_index': 'ynet-hebrew-rag-bedrock-index',\n",
       "  '_id': '1%3A0%3AD_wYQ4sBC56RD6t2IxD7',\n",
       "  '_score': 0.0036845317,\n",
       "  'fields': {'text': ['שיעור האבטלה בישראל המשיך לרדת גם בחודש ספטמבר לשיעור של 3.4%, אולם נראה שזה יהיה השיא הנמוך האחרון לתקופה הקרובה. כעת חוזים במשק עליה חדה במיוחד בשיעור האבטלה, בשל הפגיעה הקשה בעסקים ובחברות עקב מלחמת \"חרבות ברזל\", שפרצה כבר בשבוע הראשון של חודש אוקטובר. האבטלה בישראל היא כיום עדיין מהנמוכות בעולם, אך צפויה להיות שוב גבוהה בימי המלחמה.     לפי שעה אין עדיין הערכות ביחס לשיעור הגידול, אולם ידוע כבר על עשרות אלפי עובדים שנשלחו לחופשת בבית, בין השאר מענפי התיירות, הבידור והמסעדנות, ולפי שעה אין כוונה במשרדי האוצר ובביטוח הלאומי לחזור לנוסחת החל\"ת מימי משבר הקורונה. בביטוח הלאומי מסבירים כי העובדים יפסידו חלק ניכר ממשכורתם אם תאומץ שיטת החל\"ת מהקורונה, מה עוד שעובדים רבים טרם צברו את תקופת האכשרה של 12 חודשי עבודה הדרושים מתוך 18 החודשים האחרונים כדי לקבל עד 70% דמי אבטלה (דהיינו שנת עבודה דרושה מאז אפריל 2022). מנתוני הלמ\"ס שפורסמו היום עולה כי בספטמבר היו בישראל  151.1 אלף מובטלים, שהם 3.4% מהעובדים והמבקשים לעבוד במשק. זאת, לעומת 155.9 אלף מובטלים, שהיו 3.5% באוגוסט השנה. על פי הנתונים יחד עם המועסקים שנעדרו זמנית מעבודתם היו בספטמבר 163.6 בלתי מועסקים, שהם 3.6%, בהשוואה ל-171.0 מובטלים, שהם 3.8% באוגוסט. בצירוף עובדים שפוטרו בשל סגירת מקום העבודה בשנתיים האחרונות (עדיין בעת משבר הקורונה) שיעור האבטלה בספטמבר הוא 4.0%, שהם 181.5 אלף, לעומת 186.0 אלף, שהם 4.1% באוגוסט. יחד עם מי שכבר התייאשו מלמצוא עבודה הגיע שיעור האבטלה המורחב ביותר בספטמבר ל-4.2%, שהם 191.5 אלף, לעומת באוגוסט 4.5% מובטלים, שהם 205.3 אלף ישראלים, דהיינו עדיין מעל מאתיים אלף מובטלים. בסך הכל היו בישראל בספטמבר 7,085,300 בני 15 ומעלה שיכולים היו לעבוד, לפי ההגדרות הבינלאומיות. מתוכם היו בכוח העבודה 4,493,800 ישראלים, שהם 63.4%, לעומת 2,591,500 נשים וגברים שלא נכללו בכוח העבודה, כיוון שלא עבדו ולא חיפשו כלל עבודה בחודש ספטמבר.']}},\n",
       " {'_index': 'ynet-hebrew-rag-bedrock-index',\n",
       "  '_id': '1%3A0%3ANvwYQ4sBC56RD6t2IxD7',\n",
       "  '_score': 0.0036797945,\n",
       "  'fields': {'text': ['השקל נחלש הבוקר (רביעי) מול השקל ביום ה-12 של מלחמת \"חרבות ברזל\" ונסחר סביב 4.02 שקלים, עלייה של כ-0.4 אחוז. האירו עולה גם כן ונסחר סביב 4.25 שקלים. השער היציג של הדולר נקבע היום על 4.02 שקלים ושל האירו על 4.24 שקלים. כזכור, גם אתמול הדולר נסחר סביב 4 שקלים, אבל בהמשך הגיב בירידה ל-3.99 שקלים, ככל הנראה בעקבות רמזים של בנק ישראל לפיהם לא תהיה הורדת ריבית בשבוע הבא. בדומה לאתמול, גם היום המדדים המובילים בבורסה בתל אביב בירידה קלה של כ-0.1%.  >> לסיפורים החשובים והמעניינים בכלכלה ובצרכנות - הצטרפו לערוץ הטלגרם שלנו, האזינו לפודקאסט הכלכלי היומי \"כסף חדש\", וסמנו \"כלכלה\" בהתראות אפליקציית ynet   כאמור, אתמול בצהריים פרסם בנק ישראל הודעה לפיה המשנה לנגיד, אנדרו אביר, קיים היום מפגש מקצועי עם החזאים הפיננסיים, בהמשך לתדרוך העיתונאים שנערך ובו תיאר את תכנית מכירת המט\"ח שהפעיל הבנק לפני מספר ימים. לפי ההודעה, המשנה לנגיד אמר כי: \"מדיניות בנק ישראל מתמקדת בייצוב השווקים ויצירת ודאות מקסימלית למשק ולציבור בעת הזו. הבנק יצא עם כלי ייעודי לייצוב התנודות בשוק המט\"ח שתרם לייצוב ורגיעה בשווקים נוספים – ושואף כי יתר כלי המדיניות המוניטרית לא יאתגרו תכלית זו בטווח המיידי. כפי שאמר הנגיד השבוע בנאומו ל-G30 \"הסיכון המרכזי לאינפלציה בתשעת החודשים האחרונים, וכעת ביתר שאת, הוא הפיחות של השקל\". משמעות ההודעה היא שבנק ישראל ככל הנראה לא רואה צורך אמיתי בהורדת ריבית - בטח לא מידית. פיץ\\': כלכלת ישראל תחת \"מעקב שלילי\" כמו כן, חברת דירוג האשראי הבינלאומית פיץ\\' הודיעה אמש על הצבת דירוג האשראי של מדינת ישראל, העומד על רמה של +A, תחת \"מעקב שלילי\" (Rating Watch Negative), זאת בשל שינוי בתפיסת הסיכון הגיאופוליטי בעקבות המלחמה.  משמעות ההודעה היא שבכוונת החברה לעקוב באופן תדיר אחר ההתפתחויות הביטחוניות באזור, ובמהלך ששת החודשים הקרובים, במקרה של הרעה משמעותית במצב הביטחוני, היא עלולה לבצע פעולת דירוג שלילית. לחילופין, היה והרעה כאמור לא תתממש, יוסר \"המעקב השלילי\". ההודעה הנוכחית אינה מהווה הורדת דירוג או הורדת תחזית הדירוג. יוסי פריימן, מנכ\"ל פריקו ניהול סיכונים, מימון והשקעות, הסביר היום כי \"יציאת משקיעים מהחזקה בנכסיים שיקליים לצד ביקושים מקומיים תורמת לעודפי הביקוש למט\"ח ולפיחות השקל. ההיצע']}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = open_search_serverless_client.search(body=vector_query, index=index_name)\n",
    "response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "You have reached the end of this workshop. Following cell will delete all created resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c287002c-a8ee-4112-9e0e-f69847269843',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c287002c-a8ee-4112-9e0e-f69847269843',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '2',\n",
       "   'date': 'Wed, 18 Oct 2023 14:15:41 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aoss_client.delete_collection(id=collection['createCollectionDetail']['id'])\n",
    "aoss_client.delete_access_policy(name=access_policy_name, type='data')\n",
    "aoss_client.delete_security_policy(name=encryption_policy_name, type='encryption')\n",
    "aoss_client.delete_security_policy(name=network_policy_name, type='network')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this moduel on retrieval augmented generation! This is an important technique that combines the power of large language models with the precision of retrieval methods. By augmenting generation with relevant retrieved examples, the responses we recieved become more coherent, consistent and grounded. You should feel proud of learning this innovative approach. I'm sure the knowledge you've gained will be very useful for building creative and engaging language generation systems. Well done!\n",
    "\n",
    "In the above implementation of RAG based Question Answering we have explored the following concepts and how to implement them using Amazon Bedrock and it's LangChain integration.\n",
    "\n",
    "- Loading documents and generating embeddings to create a vector store\n",
    "- Retrieving documents to the question\n",
    "- Preparing a prompt which goes as input to the LLM\n",
    "- Present an answer in a human friendly manner\n",
    "- keep source knowledge up to date, and improve trust in our system by providing citations with every answer.\n",
    "\n",
    "### Take-aways\n",
    "- Experiment with different Vector Stores\n",
    "- Leverage various models available under Amazon Bedrock to see alternate outputs\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores\n",
    "\n",
    "# Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
